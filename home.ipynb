{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff45b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a subset of OPUS-100\n",
    "dataset = load_dataset(\"opus100\", \"ar-en\")\n",
    "\n",
    "# Use only the first 10,000 examples for training and 2,000 for validation\n",
    "train_data = dataset[\"train\"].select(range(10_000))\n",
    "valid_data = dataset[\"validation\"].select(range(2_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89f39bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation'],\n",
      "    num_rows: 1000000\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'translation': {'ar': 'مقرف', 'en': 'Ugh. Disgusting.'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[\"train\"])\n",
    "dataset[\"train\"][5]  # Show first sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7580d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_translation(example):\n",
    "    en = example[\"translation\"][\"en\"]\n",
    "    ar = example[\"translation\"][\"ar\"]\n",
    "\n",
    "    # Remove samples with empty or very short sentences\n",
    "    if not en.strip() or not ar.strip():\n",
    "        return False\n",
    "    if len(en.strip().split()) < 2 or len(ar.strip().split()) < 2:\n",
    "        return False\n",
    "\n",
    "    # Filter if identical\n",
    "    if en.strip() == ar.strip():\n",
    "        return False\n",
    "\n",
    "    # Optional: filter based on language detection or non-Arabic characters\n",
    "    return True\n",
    "\n",
    "# Apply filtering\n",
    "cleaned_dataset = dataset.filter(clean_translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8284cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\Documents\\LEVEL 4\\TERM 2\\NLP\\PROJECT\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pc\\Documents\\LEVEL 4\\TERM 2\\NLP\\PROJECT\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer , EarlyStoppingCallback\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex['en'] for ex in examples['translation']]\n",
    "    targets = [ex['ar'] for ex in examples['translation']]\n",
    "    \n",
    "    # Tokenize inputs with padding\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Tokenize labels with padding\n",
    "    labels = tokenizer(\n",
    "        targets,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_valid = valid_data.map(preprocess_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd2b975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Prepare labels for sacrebleu format: List of List of references\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1522fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ar\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",          #  REQUIRED for early stopping + checkpointing\n",
    "    save_strategy=\"steps\",                #  Optional but defaults to \"steps\"\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",   # REQUIRED for EarlyStopping\n",
    "    greater_is_better=False,             #  lower eval_loss = better\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "533787b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, \n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af69db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3750 [00:00<?, ?it/s]c:\\Users\\pc\\Documents\\LEVEL 4\\TERM 2\\NLP\\PROJECT\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "  3%|▎         | 100/3750 [03:02<1:58:07,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8443, 'grad_norm': 2.322962760925293, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 200/3750 [06:43<2:27:29,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4363, 'grad_norm': 1.7182132005691528, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 300/3750 [10:47<2:19:57,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4136, 'grad_norm': 1.4067925214767456, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 400/3750 [14:49<2:19:52,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.41, 'grad_norm': 2.1516456604003906, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 500/3750 [18:14<1:49:40,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4319, 'grad_norm': 2.3079283237457275, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "192690e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Prepare labels for sacrebleu format: List of List of references\n",
    "    decoded_labels = [[label] for label in decoded_labels]\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": result[\"score\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e929bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49a67050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[62801]], 'forced_eos_token_id': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('en-ar-model\\\\tokenizer_config.json',\n",
       " 'en-ar-model\\\\special_tokens_map.json',\n",
       " 'en-ar-model\\\\vocab.json',\n",
       " 'en-ar-model\\\\source.spm',\n",
       " 'en-ar-model\\\\target.spm',\n",
       " 'en-ar-model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"en-ar-model\")\n",
    "tokenizer.save_pretrained(\"en-ar-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ab2972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer , pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8b442db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pc\\Documents\\LEVEL 4\\TERM 2\\NLP\\PROJECT\\.venv\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"en-ar-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"en-ar-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccc2468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Hugging Face pipeline for simplicity\n",
    "translator = pipeline(\"translation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Example English-Arabic pairs (can be from OPUS-100 test set)\n",
    "samples = [\n",
    "    {\"en\": \"I love learning new languages.\", \"ar\": \"أحب تعلم لغات جديدة.\"},\n",
    "    {\"en\": \"Can you help me with my homework?\", \"ar\": \"هل يمكنك مساعدتي في واجبي؟\"},\n",
    "]\n",
    "\n",
    "# Translate and collect predictions and references\n",
    "predictions = []\n",
    "references = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a6f128c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Sample Results:\n",
      "EN: The weather is nice today.\n",
      "PRED: الأمر جيد اليوم\n",
      "REF: الطقس جميل اليوم.\n",
      "\n",
      "EN: I love learning new languages.\n",
      "PRED: أحب تعلم لغات جديدة\n",
      "REF: أحب تعلم لغات جديدة.\n",
      "\n",
      "EN: Can you help me with my homework?\n",
      "PRED: هل يمكنك مساعدتي بفراضي؟\n",
      "REF: هل يمكنك مساعدتي في واجبي؟\n",
      "\n",
      "🌍 BLEU Score: 46.26\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    translated = translator(sample[\"en\"], max_length=128)[0][\"translation_text\"]\n",
    "    predictions.append(translated)\n",
    "    references.append([sample[\"ar\"]])  # BLEU expects a list of references\n",
    "\n",
    "# Compute BLEU score\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"\\n📝 Sample Results:\")\n",
    "for src, pred, ref in zip([s[\"en\"] for s in samples], predictions, references):\n",
    "    print(f\"EN: {src}\")\n",
    "    print(f\"PRED: {pred}\")\n",
    "    print(f\"REF: {ref[0]}\\n\")\n",
    "\n",
    "print(f\"🌍 BLEU Score: {results['score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1549ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate():\n",
    "    input_text = text_input.get(\"1.0\", tk.END).strip()\n",
    "    if not input_text:\n",
    "        return\n",
    "    direction = language_var.get()\n",
    "\n",
    "    if direction == \"English → Arabic\":\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    else:\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True, src_lang=\"arb\", tgt_lang=\"eng\")\n",
    "\n",
    "    output = model.generate(**inputs, max_length=128)\n",
    "    translated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    text_output.config(state=\"normal\")\n",
    "    text_output.delete(\"1.0\", tk.END)\n",
    "    text_output.insert(tk.END, translated_text)\n",
    "    text_output.config(state=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "914bc3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI Setup\n",
    "root = tk.Tk()\n",
    "root.title(\"English --> Arabic Translator\")\n",
    "\n",
    "# Input field\n",
    "ttk.Label(root, text=\"Enter Text:\").pack(pady=5)\n",
    "text_input = tk.Text(root, height=5, width=60)\n",
    "text_input.pack(pady=5)\n",
    "\n",
    "# Direction selector\n",
    "language_var = tk.StringVar(value=\"English → Arabic\")\n",
    "direction_menu = ttk.Combobox(root, textvariable=language_var, values=[\"English → Arabic\"])\n",
    "direction_menu.pack(pady=5)\n",
    "\n",
    "# Translate button\n",
    "translate_button = ttk.Button(root, text=\"Translate\", command=translate)\n",
    "translate_button.pack(pady=10)\n",
    "\n",
    "# Output field\n",
    "ttk.Label(root, text=\"Translation:\").pack(pady=5)\n",
    "text_output = tk.Text(root, height=5, width=60, state=\"disabled\")\n",
    "text_output.pack(pady=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beba1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2595b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
